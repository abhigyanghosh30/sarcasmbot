{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, Embedding, LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow\n",
    "tensorflow.debugging.set_log_device_placement(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_labels(row):\n",
    "    if row[\"label\"] == 1:\n",
    "        return \"Negative\"\n",
    "    return \"Positive\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Function for classifying sentences into positive and negative\"\"\"\n",
    "    model_dir = \"./models\"\n",
    "\n",
    "    # Data reading and pre-processing\n",
    "    data = pd.read_csv('sentiment_data.csv')\n",
    "    data.columns = [\"text\", \"label\"]\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "    data[\"sentiment\"] = data.apply(classification_labels, axis=1)\n",
    "\n",
    "    data.drop(\"label\", axis=1, inplace=True)\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "    data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
    "\n",
    "    maxlen = 30\n",
    "    max_features = 20000\n",
    "    tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "    tokenizer.fit_on_texts(data['text'].values)\n",
    "    X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "    X = pad_sequences(X, maxlen=maxlen)\n",
    "\n",
    "    token_path = os.path.join(model_dir, \"token_\" + \"v1\")\n",
    "    complete_token_path = token_path + \".pickle\"\n",
    "    with open(complete_token_path, 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(X.shape)\n",
    "\n",
    "    embed_dim = 128\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    emb1 = Embedding(max_features, embed_dim, mask_zero=True)(inputs)\n",
    "    lstm1 = LSTM(200, return_sequences=True)(emb1)\n",
    "    lstm_out, att_weights = SeqSelfAttention(attention_activation='sigmoid', return_attention=True)(lstm1)\n",
    "    lstm2 = LSTM(150, return_sequences=False, trainable=False)(lstm_out)\n",
    "    outputs = Dense(2, activation='sigmoid')(lstm2)\n",
    "    model = Model(inputs=[inputs], outputs=outputs)\n",
    "    print(model.summary())\n",
    "\n",
    "    Y = pd.get_dummies(data['sentiment']).values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    print(x_test.shape, y_test.shape)\n",
    "    batch_size = 32\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model_name = \"dummy_model_\" + \"v1\" + \"_embed_\" + str(embed_dim) + \"_batch_size_\" + str(32)\n",
    "    filepath = os.path.join(model_dir, model_name) + \"_intermediate.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1,\n",
    "                                 save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    epochs = 10\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test), verbose=1, callbacks=callbacks_list)\n",
    "\n",
    "    print(\"Training has finished. Model save at \", os.path.join(model_dir, model_name) + \"_final.hdf5\")\n",
    "    model.save(os.path.join(model_dir, model_name) + \"_final.hdf5\")\n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model, Model\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def main():\n",
    "    \"\"\"Function for removing sentiment words from a given text\"\"\"\n",
    "\n",
    "    data = pd.read_csv('dataSampleTest.csv')\n",
    "    data.columns = [\"sarcasmText\", \"text\"] # Keeping only the neccessary columns\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "    data['text_without_stopwords'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "    data['text_without_stopwords'] = data['text_without_stopwords'].apply(lambda x: x.lower())\n",
    "    data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "    model_dir = \"./models\"\n",
    "    token_path = os.path.join(model_dir, \"token_\" + \"v1\")\n",
    "    complete_token_path = token_path + \".pickle\"\n",
    "    with open(complete_token_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    maxlen =30\n",
    "    X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "    X = pad_sequences(X, maxlen=maxlen)\n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    print(X.shape)\n",
    "\n",
    "    max_features = 20000\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', n_values=max_features, sparse=False)\n",
    "    \n",
    "    x_train_one_hot = enc.fit_transform(X)\n",
    "    x_train_one_hot = np.reshape(x_train_one_hot, (X.shape[0], maxlen, max_features))\n",
    "\n",
    "    model_name = \"dummy_model_\" + \"v1\" + \"_embed_\" + str(128) + \"_batch_size_\" + str(32)\n",
    "    trained_model = os.path.join(model_dir, model_name) + \"_final.hdf5\"\n",
    "\n",
    "    model = load_model(trained_model, custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
    "\n",
    "    feat_dir = \"./features\"\n",
    "    if not os.path.exists(feat_dir):\n",
    "        os.makedirs(feat_dir)\n",
    "    print(model.summary())\n",
    "\n",
    "    dense_model = Model(inputs=model.input, outputs=model.get_layer('seq_self_attention_1').output)\n",
    "    dense_feature, attn_weight = dense_model.predict(X)\n",
    "\n",
    "    new_data_all = np.zeros((attn_weight.shape[0], attn_weight.shape[1]))\n",
    "    for i in range(0, attn_weight.shape[0]):\n",
    "        current_max_array = attn_weight[i].max(0)\n",
    "        temp_list = []\n",
    "        for k in range(0, current_max_array.shape[0]):\n",
    "            if current_max_array[k] != 0:\n",
    "                temp_list.append(current_max_array[k])\n",
    "\n",
    "        current_mean = np.mean(temp_list)\n",
    "        current_std = np.std(temp_list)\n",
    "        num_higher = current_mean + 1*(current_std)\n",
    "        num_lower = current_mean - 1.5*(current_std)\n",
    "        high_outlier = (current_max_array <= num_higher).astype(int)\n",
    "        low_outlier = (current_max_array > num_lower).astype(int)\n",
    "        context_ones = high_outlier*low_outlier\n",
    "        new_data = X[i] * context_ones\n",
    "        new_data_all[i] = new_data\n",
    "\n",
    "    def sequence_to_text(list_of_indices):\n",
    "        words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "        return words\n",
    "\n",
    "    my_texts = list(map(sequence_to_text, new_data_all))\n",
    "    correct_sent = list(data[\"text\"])\n",
    "    all_sentences = []\n",
    "    new_training_output = []\n",
    "    for i in range(0, len(my_texts)):\n",
    "        each_new = [x for x in my_texts[i] if x is not None]\n",
    "        each_new = \" \".join(each_new)\n",
    "        if each_new != \"\":\n",
    "            rem = correct_sent[i]\n",
    "            new_training_output.append(rem)\n",
    "            all_sentences.append(each_new)\n",
    "        else:\n",
    "            print(i, correct_sent[i])\n",
    "\n",
    "    print(len(all_sentences), len(new_training_output))\n",
    "    for i in range(0, len(new_training_output)):\n",
    "        print(all_sentences[i], \" ----------- \", new_training_output[i])\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
